{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMT 574 Problem Set 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPython version:       7.8.0 (need at least 1.0)\n",
      "Numpy version:        1.16.5 (need at least 1.7.1)\n",
      "SciPy version:         1.3.1 (need at least 0.12.0)\n",
      "Pandas version:       0.25.1 (need at least 0.11.0)\n",
      "Matpltolib version:     3.1.1 (need at least 1.2.1)\n",
      "Scikit-Learn version: 0.21.3 (need at least 0.13.1)\n"
     ]
    }
   ],
   "source": [
    "#IPython is what you are using now to run the notebook\n",
    "import IPython\n",
    "print( \"IPython version:      %6.6s (need at least 1.0)\" % IPython.__version__)\n",
    "\n",
    "# Numpy is a library for working with arrays and matrices\n",
    "import numpy as np\n",
    "print( \"Numpy version:        %6.6s (need at least 1.7.1)\" % np.__version__)\n",
    "\n",
    "# SciPy implements many different numerical algorithms\n",
    "import scipy as sp\n",
    "print( \"SciPy version:        %6.6s (need at least 0.12.0)\" % sp.__version__)\n",
    "\n",
    "# Pandas makes working with data tables easier\n",
    "import pandas as pd\n",
    "print( \"Pandas version:       %6.6s (need at least 0.11.0)\" % pd.__version__)\n",
    "\n",
    "# Module for plotting\n",
    "import matplotlib.pyplot as plt  \n",
    "from pylab import *\n",
    "print( \"Matpltolib version:    %6.6s (need at least 1.2.1)\" %\n",
    "       matplotlib.__version__)\n",
    "%matplotlib inline\n",
    "# necessary for in-line graphics\n",
    "\n",
    "# SciKit Learn implements several Machine Learning algorithms\n",
    "import sklearn\n",
    "print(\"Scikit-Learn version: %6.6s (need at least 0.13.1)\" %\n",
    "       sklearn.__version__)\n",
    "import os\n",
    "# for certain system-related functions\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Explore and clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take a look at a few lines of data (you may use pd.sample for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13442, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>link</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11567</td>\n",
       "      <td>Frank S. Nugent</td>\n",
       "      <td>fresh</td>\n",
       "      <td>31762</td>\n",
       "      <td>http://movies.nytimes.com/movie/review?res=9E0...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>It's a fairly good melodrama, nothing more.</td>\n",
       "      <td>2003-05-20 00:00:00</td>\n",
       "      <td>19047</td>\n",
       "      <td>Only Angels Have Wings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11591</td>\n",
       "      <td>Variety Staff</td>\n",
       "      <td>fresh</td>\n",
       "      <td>63823</td>\n",
       "      <td>http://www.variety.com/review/VE1117796524.htm...</td>\n",
       "      <td>Variety</td>\n",
       "      <td>Here are all the ingredients of a novel entert...</td>\n",
       "      <td>2007-09-06 00:00:00</td>\n",
       "      <td>9408</td>\n",
       "      <td>Yellow Submarine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10419</td>\n",
       "      <td>Roger Ebert</td>\n",
       "      <td>rotten</td>\n",
       "      <td>141098</td>\n",
       "      <td>http://www.rogerebert.com/reviews/forces-of-na...</td>\n",
       "      <td>Chicago Sun-Times</td>\n",
       "      <td>A romantic shaggy dog story, a movie that lead...</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>11350</td>\n",
       "      <td>Forces of Nature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1449</td>\n",
       "      <td>James Berardinelli</td>\n",
       "      <td>rotten</td>\n",
       "      <td>114594</td>\n",
       "      <td>http://www.reelviews.net/movies/s/swimming.html</td>\n",
       "      <td>ReelViews</td>\n",
       "      <td>There doesn't seem to be enough material here ...</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>15161</td>\n",
       "      <td>Swimming with Sharks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10965</td>\n",
       "      <td>Owen Gleiberman</td>\n",
       "      <td>fresh</td>\n",
       "      <td>99052</td>\n",
       "      <td>http://www.ew.com/ew/article/0,,317842,00.html</td>\n",
       "      <td>Entertainment Weekly</td>\n",
       "      <td>It gives you the willies in a cheery, presenta...</td>\n",
       "      <td>1990-07-18 00:00:00</td>\n",
       "      <td>11047</td>\n",
       "      <td>Arachnophobia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   critic   fresh    imdb  \\\n",
       "11567     Frank S. Nugent   fresh   31762   \n",
       "11591       Variety Staff   fresh   63823   \n",
       "10419         Roger Ebert  rotten  141098   \n",
       "1449   James Berardinelli  rotten  114594   \n",
       "10965     Owen Gleiberman   fresh   99052   \n",
       "\n",
       "                                                    link  \\\n",
       "11567  http://movies.nytimes.com/movie/review?res=9E0...   \n",
       "11591  http://www.variety.com/review/VE1117796524.htm...   \n",
       "10419  http://www.rogerebert.com/reviews/forces-of-na...   \n",
       "1449     http://www.reelviews.net/movies/s/swimming.html   \n",
       "10965     http://www.ew.com/ew/article/0,,317842,00.html   \n",
       "\n",
       "                publication  \\\n",
       "11567        New York Times   \n",
       "11591               Variety   \n",
       "10419     Chicago Sun-Times   \n",
       "1449              ReelViews   \n",
       "10965  Entertainment Weekly   \n",
       "\n",
       "                                                   quote          review_date  \\\n",
       "11567        It's a fairly good melodrama, nothing more.  2003-05-20 00:00:00   \n",
       "11591  Here are all the ingredients of a novel entert...  2007-09-06 00:00:00   \n",
       "10419  A romantic shaggy dog story, a movie that lead...  2000-01-01 00:00:00   \n",
       "1449   There doesn't seem to be enough material here ...  2000-01-01 00:00:00   \n",
       "10965  It gives you the willies in a cheery, presenta...  1990-07-18 00:00:00   \n",
       "\n",
       "        rtid                   title  \n",
       "11567  19047  Only Angels Have Wings  \n",
       "11591   9408        Yellow Submarine  \n",
       "10419  11350        Forces of Nature  \n",
       "1449   15161    Swimming with Sharks  \n",
       "10965  11047           Arachnophobia  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df= pd.read_csv(\"rotten-tomatoes.csv.bz2\")\n",
    "print(movie_df.shape)\n",
    "movie_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print out all variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['critic', 'fresh', 'imdb', 'link', 'publication', 'quote',\n",
       "       'review_date', 'rtid', 'title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a summary table (maybe more like a bullet list) where you print out the most important summary statistics for the most interesting variables. The most interesting facts you should present should include: a) number of missings for fresh and quote; b) all different values for fresh/rotten evaluations; c) counts or percentages of these values; d) number of zero-length or only whitespace quote-s; e) minimum-maximum-average length of quotes (either in words, or in characters). (Can you do this as an one-liner?); f) how many reviews are in data multiple times. Feel free to add more figures you consider relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_fresh=movie_df.fresh.isna().sum()\n",
    "null_quote=movie_df.quote.isna().sum()\n",
    "diff_val=movie_df.fresh.unique()\n",
    "per_fresh=round((movie_df.fresh==\"fresh\").sum()*100/len(movie_df.fresh),2)\n",
    "per_rot=round((movie_df.fresh==\"rotten\").sum()*100/len(movie_df.fresh),2)\n",
    "per_none=round((movie_df.fresh==\"none\").sum()*100/len(movie_df.fresh),2)\n",
    "num_zero= movie_df.quote.isnull().sum() + movie_df[movie_df.quote.str.len()==0].shape[0]\n",
    "num_white_space = round(movie_df.quote==\" \").sum() + movie_df[movie_df.quote.str.isspace()].shape[0]\n",
    "repeat_quotes = movie_df.quote.shape[0] + movie_df.quote.unique().size\n",
    "max_quote = movie_df.quote.map(lambda x: len(x)).max()\n",
    "min_quote = movie_df.quote.map(lambda x: len(x)).min()\n",
    "avg_quote = round(movie_df.quote.map(lambda x: len(x)).mean(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values for the 'fresh' variable is 0\n",
      "Number of missing values for the 'quote' variable is 0\n",
      "All different values for fresh/rotten evaluations are fresh, rotten and none.\n",
      "Percentage of fresh values is 62.41 %\n",
      "Percentage of rotten values is 37.42 %\n",
      "Percentage of none values is 0.17 %\n",
      "Number of zero-length quotes is 0\n",
      "Number of only whitespace quotes is 0.0\n",
      "Minimum length of quotes (in characters) is 4\n",
      "Maximum length of quotes (in characters) is 256\n",
      "Average length of quotes (in characters) is 121.23\n",
      "Number of reviews present multiple times in the data is  26278\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of missing values for the 'fresh' variable is {:d}\".format(null_fresh))\n",
    "print(\"Number of missing values for the 'quote' variable is {:d}\".format(null_quote))\n",
    "print(\"All different values for fresh/rotten evaluations are\", diff_val[0]+ \",\",diff_val[1], \"and\",diff_val[2]+\".\")\n",
    "print(\"Percentage of fresh values is\",per_fresh,\"%\")\n",
    "print(\"Percentage of rotten values is\",per_rot, \"%\")\n",
    "print(\"Percentage of none values is\", per_none,\"%\")\n",
    "print(\"Number of zero-length quotes is\", num_zero)\n",
    "print(\"Number of only whitespace quotes is\", num_white_space)\n",
    "print(\"Minimum length of quotes (in characters) is\", min_quote)\n",
    "print(\"Maximum length of quotes (in characters) is\", max_quote)\n",
    "print(\"Average length of quotes (in characters) is\", avg_quote)\n",
    "print(\"Number of reviews present multiple times in the data is \",repeat_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now when you have an overview what you have in data, clean it by removing all the inconsistencies the table reveals. We have to ensure that the central variables, quote and fresh are not missing, quote is not an empty string (or just contain spaces and such), and all rows are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df_clean=movie_df.drop_duplicates() #dropping duplicate data\n",
    "movie_df_new=movie_df_clean[movie_df_clean.fresh!=\"none\"] #removing reviews which are neither fresh nor rotten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convert your data (quotes) into bag-of-words. Your code should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(movie_df_new.quote.values)\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the targer variable\n",
    "y= movie_df_new['fresh'].values\n",
    "y = np.where(y == 'fresh', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split your work data and target (i.e. the variable fresh) into training and validation chunks (80/20 or so)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X_train,X_valid, y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now you are ready with the preparatory work and it's time to dive into the real thing. Let'simplement Naive Bayes. Use only training data when doing the fitting below.\n",
    "\n",
    "4. Compute the unconditional (log) probability that the tomato is fresh/rotten, log Pr(F), and log Pr(R). These probabilities are based on the values of fresh alone, not on the words the quotes contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4766154175778479"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the logPr(F)\n",
    "log_pr_f = np.log(y_train[y_train==1].shape[0]/y_train.shape[0])\n",
    "log_pr_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9699058337934665"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the logPr(R)\n",
    "log_pr_r = np.log(y_train[y_train==0].shape[0]/y_train.shape[0])\n",
    "log_pr_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. For each word w, compute log Pr(wjF) and log Pr(wjR), the (log) probability that the word is present in a fresh/rotten review. These probabilities can easily be calculated from counts of how many times these words are present for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh = y_train[y_train==1].shape[0]\n",
    "rotten = y_train[y_train==0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.array(range(len(y_train)))\n",
    "fresh_index = []\n",
    "rotten_index = []\n",
    "for i in index:\n",
    "    if y_train[i] == 0:\n",
    "        rotten_index.append(i)\n",
    "    if y_train[i] == 1:\n",
    "        fresh_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_words  = X_train[fresh_index]\n",
    "rotten_words = X_train[rotten_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_w_f = np.sum(fresh_words, axis =0 )/ fresh \n",
    "pr_w_r = np.sum(rotten_words, axis = 0) / rotten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_w_f = pd.Series(np.array(pr_w_f).ravel())\n",
    "pr_w_r = pd.Series(np.array(pr_w_r).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the log probability that the word is present in a fresh/rotten review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pr_w_f = pr_w_f.apply(lambda x: np.log(x) if x!=0 else 0)\n",
    "log_pr_w_r = pr_w_r.apply(lambda x: np.log(x) if x!=0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For both destination classes, F and R, compute the log-likelihood that the quote belongs to this class. Computing these likelihoods involves sums of the previously computed probabilities, log Pr(wjF), and BOW elements xij. Check out np.apply_along_axis that can be used to apply a function on matrix columns/rows so you can create a fairly good one-liner to compute log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_vectorized = pd.DataFrame(X_valid.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "\n",
    "def log_likelihood(ith_review):\n",
    "    log_fresh_sum = 0\n",
    "    log_rotten_sum = 0\n",
    "   \n",
    "    for j in range(ith_review.shape[0]):\n",
    "        if (ith_review[j]!=0):\n",
    "\n",
    "            log_fresh_sum =  log_fresh_sum + (log_pr_w_f[j])\n",
    "            log_rotten_sum =  log_rotten_sum + ((log_pr_w_r[j]))\n",
    "\n",
    "    log_likelihood_fresh_ith_review = log_fresh_sum + log_pr_f\n",
    "    log_likelihood_rotten_ith_review = log_rotten_sum + log_pr_r\n",
    "    \n",
    "    if  log_likelihood_fresh_ith_review > log_likelihood_rotten_ith_review:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, None, None, ..., None, None, None], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.apply_along_axis(log_likelihood, 1, X_valid_vectorized )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Print the resulting confusion matrix and accuracy (feel free to use existing libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[624 355]\n",
      " [976 610]]\n",
      "Accuracy of the classifier is 48.11 %\n"
     ]
    }
   ],
   "source": [
    "pred = np.array(prediction)\n",
    "print(\"Confusion matrix:\")\n",
    "print( confusion_matrix(y_valid ,pred))\n",
    "print(\"Accuracy of the classifier is\" , round((accuracy_score(y_valid, pred)*100),2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Interpretation\n",
    "\n",
    "Now it is time to look at your fitted model a little bit closer. NB model probabilities are rather easy to understand and interpret. The task here is to and the best words to predict a fresh, and a rotten review. And we only want to look at words that are reasonably frequent, say more frequent than 30 times in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract from your conditional probability vectors log Pr(wjF) and log Pr(wjR) the probabilities that correspond to frequent words only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_occurences_of_each_word = pd.DataFrame(X_train.sum(axis=0))\n",
    "#selecting words with frequency greater than 30\n",
    "sum_of_occurences_of_each_word=sum_of_occurences_of_each_word.ix[:,sum_of_occurences_of_each_word.gt(31).any()] \n",
    "sum_of_occurences_of_each_word=sum_of_occurences_of_each_word.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Word Frequency</th>\n",
       "      <th>log_pr_w_f</th>\n",
       "      <th>log_pr_w_r</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>about</td>\n",
       "      <td>652.0</td>\n",
       "      <td>-2.805954</td>\n",
       "      <td>-2.678659</td>\n",
       "      <td>-0.127296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>above</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-5.463361</td>\n",
       "      <td>-5.626850</td>\n",
       "      <td>0.163489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>achievement</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-5.426993</td>\n",
       "      <td>-6.879613</td>\n",
       "      <td>1.452620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>across</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-6.194248</td>\n",
       "      <td>-5.321468</td>\n",
       "      <td>-0.872780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>act</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-5.986609</td>\n",
       "      <td>-5.221385</td>\n",
       "      <td>-0.765224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20767</td>\n",
       "      <td>years</td>\n",
       "      <td>113.0</td>\n",
       "      <td>-4.215903</td>\n",
       "      <td>-5.321468</td>\n",
       "      <td>1.105565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20775</td>\n",
       "      <td>yet</td>\n",
       "      <td>130.0</td>\n",
       "      <td>-4.281861</td>\n",
       "      <td>-4.528238</td>\n",
       "      <td>0.246377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20792</td>\n",
       "      <td>you</td>\n",
       "      <td>677.0</td>\n",
       "      <td>-2.728512</td>\n",
       "      <td>-2.701387</td>\n",
       "      <td>-0.027126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20793</td>\n",
       "      <td>young</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-4.648324</td>\n",
       "      <td>-5.047032</td>\n",
       "      <td>0.398708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20798</td>\n",
       "      <td>your</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-4.352479</td>\n",
       "      <td>-4.353884</td>\n",
       "      <td>0.001406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>738 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word  Word Frequency  log_pr_w_f  log_pr_w_r  Difference\n",
       "265          about           652.0   -2.805954   -2.678659   -0.127296\n",
       "266          above            41.0   -5.463361   -5.626850    0.163489\n",
       "352    achievement            32.0   -5.426993   -6.879613    1.452620\n",
       "376         across            32.0   -6.194248   -5.321468   -0.872780\n",
       "377            act            37.0   -5.986609   -5.221385   -0.765224\n",
       "...            ...             ...         ...         ...         ...\n",
       "20767        years           113.0   -4.215903   -5.321468    1.105565\n",
       "20775          yet           130.0   -4.281861   -4.528238    0.246377\n",
       "20792          you           677.0   -2.728512   -2.701387   -0.027126\n",
       "20793        young            86.0   -4.648324   -5.047032    0.398708\n",
       "20798         your           132.0   -4.352479   -4.353884    0.001406\n",
       "\n",
       "[738 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=pd.DataFrame(words)\n",
    "log_pr_w_f_df=pd.DataFrame(log_pr_w_f)\n",
    "log_pr_w_r_df=pd.DataFrame(log_pr_w_r)\n",
    "new_tab=pd.concat([words,sum_of_occurences_of_each_word,log_pr_w_f_df,log_pr_w_r_df], axis=1)\n",
    "new_tab=new_tab.dropna()\n",
    "new_tab.columns=[\"Word\",\"Word Frequency\",\"log_pr_w_f\", \"log_pr_w_r\"]\n",
    "new_tab[\"Difference\"]=log_pr_w_f_df-log_pr_w_r_df\n",
    "new_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find 10 best words to predict F and 10 best words to predict R. Hint: imagine we have a review that contains just a single word. Which word will give the highest weight to the probability the review is fresh? Which one to the likelihood it is rotten?Comment your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "From the below table, we can see that words like \"masterpiece\", \"touching\", \"extraordinary\" etc. are the best words to predict a fresh review as for these words, the probability of them being fresh is large and at the same time, the probability of them being rotten is small (which can be seen as the difference between log Pr(w|F) and log Pr(w|R) is the largest for these words). Also, logically these words have a positive sentiment and would surely be present in a fresh review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Word Frequency</th>\n",
       "      <th>log_pr_w_f</th>\n",
       "      <th>log_pr_w_r</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11319</td>\n",
       "      <td>masterpiece</td>\n",
       "      <td>54.0</td>\n",
       "      <td>-4.807954</td>\n",
       "      <td>-7.572760</td>\n",
       "      <td>2.764806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18919</td>\n",
       "      <td>touching</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-5.203850</td>\n",
       "      <td>-7.572760</td>\n",
       "      <td>2.368910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6525</td>\n",
       "      <td>extraordinary</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-5.232837</td>\n",
       "      <td>-7.572760</td>\n",
       "      <td>2.339923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14821</td>\n",
       "      <td>recent</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-5.070318</td>\n",
       "      <td>-7.167295</td>\n",
       "      <td>2.096977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3666</td>\n",
       "      <td>complex</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-5.175679</td>\n",
       "      <td>-7.167295</td>\n",
       "      <td>1.991616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20595</td>\n",
       "      <td>witty</td>\n",
       "      <td>49.0</td>\n",
       "      <td>-4.952535</td>\n",
       "      <td>-6.879613</td>\n",
       "      <td>1.927078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17913</td>\n",
       "      <td>succeeds</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-5.325211</td>\n",
       "      <td>-7.167295</td>\n",
       "      <td>1.842084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10307</td>\n",
       "      <td>kubrick</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-5.325211</td>\n",
       "      <td>-7.167295</td>\n",
       "      <td>1.842084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7494</td>\n",
       "      <td>funniest</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-5.358000</td>\n",
       "      <td>-7.167295</td>\n",
       "      <td>1.809295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8062</td>\n",
       "      <td>greatest</td>\n",
       "      <td>42.0</td>\n",
       "      <td>-5.121612</td>\n",
       "      <td>-6.879613</td>\n",
       "      <td>1.758001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word  Word Frequency  log_pr_w_f  log_pr_w_r  Difference\n",
       "11319    masterpiece            54.0   -4.807954   -7.572760    2.764806\n",
       "18919       touching            37.0   -5.203850   -7.572760    2.368910\n",
       "6525   extraordinary            36.0   -5.232837   -7.572760    2.339923\n",
       "14821         recent            43.0   -5.070318   -7.167295    2.096977\n",
       "3666         complex            39.0   -5.175679   -7.167295    1.991616\n",
       "20595          witty            49.0   -4.952535   -6.879613    1.927078\n",
       "17913       succeeds            34.0   -5.325211   -7.167295    1.842084\n",
       "10307        kubrick            34.0   -5.325211   -7.167295    1.842084\n",
       "7494        funniest            33.0   -5.358000   -7.167295    1.809295\n",
       "8062        greatest            42.0   -5.121612   -6.879613    1.758001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tab.sort_values(by=['Difference'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "From the below table, we can see that words like \"mess, \"worst\", \"lacks\", \"dull\" etc. are the best words to predict a rotten review as for these words, the probability of them being rotten is large and at the same time, the probability of them being fresh is small (which can be seen as the difference between log Pr(w|F) and log Pr(w|R) is the smallest for these words). Also, logically these words have a negative sentiment and would surely be present in a rotten review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Word Frequency</th>\n",
       "      <th>log_pr_w_f</th>\n",
       "      <th>log_pr_w_r</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12285</td>\n",
       "      <td>needs</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-6.456613</td>\n",
       "      <td>-4.898612</td>\n",
       "      <td>-1.558001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11573</td>\n",
       "      <td>mess</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-6.456613</td>\n",
       "      <td>-4.864710</td>\n",
       "      <td>-1.591903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12452</td>\n",
       "      <td>nor</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-6.561973</td>\n",
       "      <td>-4.864710</td>\n",
       "      <td>-1.697263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20676</td>\n",
       "      <td>worst</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-6.456613</td>\n",
       "      <td>-4.739547</td>\n",
       "      <td>-1.717066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10347</td>\n",
       "      <td>lacks</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-6.561973</td>\n",
       "      <td>-4.710559</td>\n",
       "      <td>-1.851414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7032</td>\n",
       "      <td>flat</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-6.679756</td>\n",
       "      <td>-4.769400</td>\n",
       "      <td>-1.910356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14188</td>\n",
       "      <td>problem</td>\n",
       "      <td>39.0</td>\n",
       "      <td>-6.813288</td>\n",
       "      <td>-4.800171</td>\n",
       "      <td>-2.013116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6593</td>\n",
       "      <td>fails</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-6.679756</td>\n",
       "      <td>-4.654989</td>\n",
       "      <td>-2.024767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>dull</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-7.372903</td>\n",
       "      <td>-4.864710</td>\n",
       "      <td>-2.508193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19543</td>\n",
       "      <td>unfortunately</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-8.759198</td>\n",
       "      <td>-4.710559</td>\n",
       "      <td>-4.048638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word  Word Frequency  log_pr_w_f  log_pr_w_r  Difference\n",
       "12285          needs            39.0   -6.456613   -4.898612   -1.558001\n",
       "11573           mess            40.0   -6.456613   -4.864710   -1.591903\n",
       "12452            nor            39.0   -6.561973   -4.864710   -1.697263\n",
       "20676          worst            44.0   -6.456613   -4.739547   -1.717066\n",
       "10347          lacks            44.0   -6.561973   -4.710559   -1.851414\n",
       "7032            flat            41.0   -6.679756   -4.769400   -1.910356\n",
       "14188        problem            39.0   -6.813288   -4.800171   -2.013116\n",
       "6593           fails            45.0   -6.679756   -4.654989   -2.024767\n",
       "5600            dull            34.0   -7.372903   -4.864710   -2.508193\n",
       "19543  unfortunately            36.0   -8.759198   -4.710559   -4.048638"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tab.sort_values(by=['Difference'], ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print out a few missclassified quotes. Can you understand why these are misclassified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see that a few quotes are misclassified between the validation set and the predicted result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "misclass=(y_valid&pred)\n",
    "print(misclass[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3616     fresh\n",
      "2188    rotten\n",
      "Name: fresh, dtype: object\n",
      "6489    fresh\n",
      "Name: fresh, dtype: object\n"
     ]
    }
   ],
   "source": [
    "y = movie_df_new['fresh']\n",
    "X_train,X_valid, y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)\n",
    "print(y_valid[5:7])\n",
    "print(y_valid[8:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassified quote 1:\n",
    "\n",
    "The below fresh review was misclassified as rotten probably because it has a strong negative word like \"wrong\" which has a higher probability of being present in rotten reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>link</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3616</td>\n",
       "      <td>J. Hoberman</td>\n",
       "      <td>fresh</td>\n",
       "      <td>53125</td>\n",
       "      <td>http://www.villagevoice.com/issues/9942/hoberm...</td>\n",
       "      <td>Village Voice</td>\n",
       "      <td>Hitchcock's ultimate wrong-man comedy.</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>18639</td>\n",
       "      <td>North by Northwest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           critic  fresh   imdb  \\\n",
       "3616  J. Hoberman  fresh  53125   \n",
       "\n",
       "                                                   link    publication  \\\n",
       "3616  http://www.villagevoice.com/issues/9942/hoberm...  Village Voice   \n",
       "\n",
       "                                       quote          review_date   rtid  \\\n",
       "3616  Hitchcock's ultimate wrong-man comedy.  2000-01-01 00:00:00  18639   \n",
       "\n",
       "                   title  \n",
       "3616  North by Northwest  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df_new[movie_df_new.index==3616]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassified quote 2:\n",
    "\n",
    "The below fresh review was misclassified as rotten probably because it has negative words like \"never\" and \"unpredictable\" which have a higher probability of being present in rotten reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>link</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6489</td>\n",
       "      <td>Roger Ebert</td>\n",
       "      <td>fresh</td>\n",
       "      <td>119738</td>\n",
       "      <td>http://www.rogerebert.com/reviews/my-best-frie...</td>\n",
       "      <td>Chicago Sun-Times</td>\n",
       "      <td>The screenplay has never been on autopilot; it...</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>12712</td>\n",
       "      <td>My Best Friend's Wedding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           critic  fresh    imdb  \\\n",
       "6489  Roger Ebert  fresh  119738   \n",
       "\n",
       "                                                   link        publication  \\\n",
       "6489  http://www.rogerebert.com/reviews/my-best-frie...  Chicago Sun-Times   \n",
       "\n",
       "                                                  quote          review_date  \\\n",
       "6489  The screenplay has never been on autopilot; it...  2000-01-01 00:00:00   \n",
       "\n",
       "       rtid                     title  \n",
       "6489  12712  My Best Friend's Wedding  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df_new[movie_df_new.index==6489]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassified quote 3:\n",
    "\n",
    "The below rotten review was misclassified as fresh probably because it has a positive word like \"heaven\" which has a higher probability of being present in fresh reviews and also the other words in this review are neutral and do not have a high probability of being strongly fresh or rotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>link</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2188</td>\n",
       "      <td>Desson Thomson</td>\n",
       "      <td>rotten</td>\n",
       "      <td>107096</td>\n",
       "      <td>http://www.washingtonpost.com/wp-srv/style/lon...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Heaven has so many themes, ranging from Buddhi...</td>\n",
       "      <td>2001-11-16 00:00:00</td>\n",
       "      <td>14946</td>\n",
       "      <td>Heaven &amp; Earth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              critic   fresh    imdb  \\\n",
       "2188  Desson Thomson  rotten  107096   \n",
       "\n",
       "                                                   link      publication  \\\n",
       "2188  http://www.washingtonpost.com/wp-srv/style/lon...  Washington Post   \n",
       "\n",
       "                                                  quote          review_date  \\\n",
       "2188  Heaven has so many themes, ranging from Buddhi...  2001-11-16 00:00:00   \n",
       "\n",
       "       rtid           title  \n",
       "2188  14946  Heaven & Earth  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df_new[movie_df_new.index==2188]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the presence of stop words in reviews such as \"the\", \"in\", \"or\", \"and\" etc. do not specifically lend themselves to fresh or rotten reviews and have an affect on the correct classification of revies. Also, words which are present with a low frequency in the reviews cannot be highly weighted as fresh or rotten and can lead to the review being misclassified. Lastly, if a review contains many unseen words on which the model has not been trained on then it can also lead to the review being misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 NB with smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create two functions: one for fitting NB model, and another to predict outcome based on the fitted model. As mentioned above, the model is fully described with 4 probabilities, so your fitting function may return such a list as the model; and the prediction function may take it as an input.\n",
    "\n",
    "2. Add smoothing to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=movie_df_new['fresh'].values\n",
    "y=np.where(y==\"fresh\",1,0)\n",
    "#X_train,X_valid, y_train, y_valid = train_test_split(X,y, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB fitting model with smoothening\n",
    "\n",
    "def fit_model(X_train, y_train, alpha):\n",
    "    log_pr_f = np.log(len(y_train[y_train==1])/len(y_train))\n",
    "    log_pr_r = np.log(len(y_train[y_train==0])/len(y_train))\n",
    "    \n",
    "    count_fresh = len(y_train[y_train==1])\n",
    "    count_rotten = len(y_train[y_train==0])\n",
    "    \n",
    "    index = np.array(range(len(y_train)))\n",
    "    fresh_index = []\n",
    "    rotten_index = []\n",
    "    for i in index:\n",
    "        if y_train[i] == 0:\n",
    "            rotten_index.append(i)\n",
    "        if y_train[i] == 1:\n",
    "            fresh_index.append(i)\n",
    "            \n",
    "    fresh_words  = X_train[fresh_index]\n",
    "    rotten_words = X_train[rotten_index]\n",
    "    \n",
    "    fresh_sum  = np.sum(fresh_words,  axis = 0 )\n",
    "    rotten_sum = np.sum(rotten_words, axis = 0)\n",
    "    \n",
    "    pr_w_f = (fresh_sum + alpha)/ (count_fresh + (alpha)) #smoothening the model\n",
    "    pr_w_r = (rotten_sum + alpha)/ (count_rotten + (alpha)) #smoothening the model\n",
    "    \n",
    "    pr_w_f = pd.Series(np.array(pr_w_f).ravel())\n",
    "    pr_w_r = pd.Series(np.array(pr_w_r).ravel())\n",
    "    \n",
    "    log_pr_w_f = pr_w_f.apply(lambda x: np.log(x) if x!=0 else 0)\n",
    "    log_pr_w_r = pr_w_r.apply(lambda x: np.log(x) if x!=0 else 0)\n",
    "    \n",
    "    return log_pr_f, log_pr_r, log_pr_w_f, log_pr_w_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict the outcome\n",
    "\n",
    "def predict_outcome(log_pr_f, log_pr_r, log_pr_w_f, log_pr_w_r, X_valid):\n",
    "    \n",
    "    X_valid_vectorized = pd.DataFrame(X_valid.toarray())\n",
    "    np.apply_along_axis(log_likelihood, 1, X_valid_vectorized)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cross-validate the accuracy (on the validation data) on a number of alpha values and and the alpha that gives you the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "\n",
    "for alpha in [0.05,0.2,0.3,0.5,0.7,0.9]:\n",
    "    \n",
    "    scores_cv = []\n",
    "    \n",
    "    for train_index, test_index in cv.split(X):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "        \n",
    "        global log_pr_f\n",
    "        global log_pr_r\n",
    "        global log_pr_w_f\n",
    "        global log_pr_w_r \n",
    "        global prediction\n",
    "        prediction = []\n",
    "        log_pr_f, log_pr_r, log_pr_w_f, log_pr_w_r = fit_model(X_train, y_train, alpha = alpha)\n",
    "        predict_outcome(log_pr_f, log_pr_r, log_pr_w_f, log_pr_w_r, X_test)\n",
    "        pred = np.array(prediction)\n",
    "        scores_cv.append(accuracy_score(y_test, pred))\n",
    "        \n",
    "    scores.append(np.mean(np.array(scores_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 73.15 % when alpha is 0.05\n",
      "The accuracy of the model is 74.12 % when alpha is 0.2\n",
      "The accuracy of the model is 74.26 % when alpha is 0.3\n",
      "The accuracy of the model is 74.2 % when alpha is 0.5\n",
      "The accuracy of the model is 73.86 % when alpha is 0.7\n",
      "The accuracy of the model is 73.39 % when alpha is 0.9\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.05,0.2,0.3,0.5,0.7,0.9]\n",
    "for i in range(len(scores)):\n",
    "    print(\"The accuracy of the model is\" , round((scores[i]*100),2),\"%\", \"when alpha is\",alpha[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that when alpha is 0.3 our model has the best accuracy of 74.26%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
